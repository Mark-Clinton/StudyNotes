<center>
    <h1>
        微服务框架
    </h1>
</center>

## 认识微服务

### 单体架构

单体架构：将业务的所有功能集中在一个项目中开发，打成一个包部署。

优点

- 架构简单
- 部署成本低

缺点

- 耦合度高

### 分布式架构

分布式架构：根据业务功能对系统进行拆分，每个业务模块作为独立项目开发，称为一个服务。

优点

- 降低服务耦合
- 有利于服务升级拓展

分布式架构要考虑的问题

- 服务拆分粒度如何？
- 服务集群地址如何维护？
- 服务之间如何实现远程调用？
- 服务健康状态如何感知？

### 微服务

微服务是一种经过良好架构设计的分布式架构方案，微服务架构有如下特征：

- 单一职责：微服务拆分粒度更小，每一个服务都对应唯一的业务能力，做到单一职责，避免重复业务开发
- 面向服务：微服务对外暴露业务接口
- 自治：团队独立、技术独立、数据独立、部署独立
- 隔离性强：服务调用做好隔离、容错、降级，避免出现级联问题

微服务技术对比

|                |        Dubbo        |       SpringCloud        |    SpringCloudAlibaba    |
| :------------: | :-----------------: | :----------------------: | :----------------------: |
|    注册中心    |  zookeeper、Redis   |      Eureka、Consul      |      Nacos、Eureka       |
|  服务远程调用  |      Dubbo协议      |     Feign(http协议)      |       Dubbo、Feign       |
|    配置中心    |         无          |    SpringCloudConfig     | SpringCloudConfig、Nacos |
|    服务网关    |         无          | SpringCloudGateway、Zuul | SpringCloudGateway、Zuul |
| 服务监控和保护 | dubbo-admin，功能弱 |         Hystrix          |         Sentinel         |

### SpringCloud

SpringCloud是目前国内使用最广泛的微服务框架。官网地址：https://spring.io/projects/spring-cloud。

SpringCloud集成了各种微服务功能组件，并基于SpringBoot实现了这些组件的自动装配，从而提供了良好的开箱即用体验。

## 服务拆分及远程调用

### 服务拆分注意事项

1.不同微服务，不要重复开发相同业务

2.微服务数据独立，不要访问其他微服务的数据库

3.微服务可以将自己的业务暴露为接口，供其它微服务调用

### 微服务远程调用

**微服务调用方式**

- 基于RestTemplate发起的http请求实现远程调用
- http请求做远程调用是与语言无关的调用，只要知道对方的ip、端口、接口路径、请求参数即可

**提供者与消费者**

- 服务提供者：一次业务中，被其他微服务调用的服务。（提供接口给其他微服务）
- 服务消费者：一次业务中，调用其它微服务的服务。（调用其它微服务提供的接口）
- 提供者与消费者角色其实是相对的
- 一个服务可以同时是服务提供者和服务消费者

## Eureka注册中心

### 服务调用出现的问题

服务消费者该如何获取服务提供者的地址信息？

如果有多个服务提供者，消费者该如何选择？

消费者如何得知服务提供者的健康状态？

### Eureka作用

服务消费者该如何获取服务提供者的地址信息？

- 服务提供者启动时向eureka注册自己的信息
- eureka保存这些信息
- 消费者根据服务名称向eureka拉取提供者信息

如果有多个服务提供者，消费者该如何选择？

- 服务消费者利用负载均衡算法，从服务列表中挑选一个

消费者如何得知服务提供者的健康状态？

- 服务提供者会每隔30秒向EurekaServer发送心跳请求，报告健康状态
- eureka会更新记录服务列表信息，心跳不正常会被剔除
- 消费者就可以拉取到最新的信息

### 微服务注册与远程调用

搭建EurekaServer，搭建步骤

1.创建项目，引入spring-cloud-starter-netflix-eureka-server的依赖

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>
</dependency>
```

2.编写启动类，添加@EnableEurekaServer注解

3.添加application.yml文件，编写下面的配置

```yaml
server:
  port: 10086
spring:
  application:
    name: eurekaserver
eureka:
  client:
    service-url:
      defaultZone: http://127.0.0.1:10086/eureka
```

注册客户端，将user-service服务注册到EurekaServer步骤如下

1.在user-service项目引入spring-cloud-starter-netflix-eureka-client的依赖

```xml
<!--Eureka客户端依赖-->
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
</dependency>
```

2.在application.yml文件，编写下面的配置

```yaml
spring:
  application:
    name: userservice
eureka:
  client:
    service-url:
      defaultZone: http://127.0.0.1:10086/eureka
```

注意：我们可以将user-service多次启动， 模拟多实例部署，但为了避免端口冲突，需要修改端口设置。

在order-service完成服务拉取

服务拉取是基于服务名称获取服务列表，然后在对服务列表做负载均衡

1.修改OrderService的代码，修改访问的url路径，用服务名代替ip、端口

```java
String url = "http://userservice/user/" + order.getUserId();
```

2.在order-service项目的启动类OrderApplication中的RestTemplate添加负载均衡注解

```java
@Bean
@LoadBalanced
public RestTemplate restTemplate() {
    return new RestTemplate();
}
```

## Ribbon负载均衡

### 负载均衡策略

Ribbon的负载均衡规则是一个叫做IRule的接口来定义的，每一个子接口都是一种规则。

- 规则接口是IRule
- 默认实现是ZoneAvoidanceRule，根据zone选择服务列表，然后轮询

|  **内置负载均衡规则类**   |                         **规则描述**                         |
| :-----------------------: | :----------------------------------------------------------: |
|      RoundRobinRule       | 简单轮询服务列表来选择服务器。它是Ribbon默认的负载均衡规则。 |
| AvailabilityFilteringRule | 对以下两种服务器进行忽略：   （1）在默认情况下，这台服务器如果3次连接失败，这台服务器就会被设置为“短路”状态。短路状态将持续30秒，如果再次连接失败，短路的持续时间就会几何级地增加。  （2）并发数过高的服务器。如果一个服务器的并发连接数过高，配置了AvailabilityFilteringRule规则的客户端也会将其忽略。并发连接数的上限，可以由客户端的\<clientName>.\<clientConfigNameSpace>.ActiveConnectionsLimit属性进行配置。 |
| WeightedResponseTimeRule  | 为每一个服务器赋予一个权重值。服务器响应时间越长，这个服务器的权重就越小。这个规则会随机选择服务器，这个权重值会影响服务器的选择。 |
|     ZoneAvoidanceRule     | 以区域可用的服务器为基础进行服务器的选择。使用Zone对服务器进行分类，这个Zone可以理解为一个机房、一个机架等。而后再对Zone内的多个服务做轮询。 |
|     BestAvailableRule     |       忽略那些短路的服务器，并选择并发数较低的服务器。       |
|        RandomRule         |                  随机选择一个可用的服务器。                  |
|         RetryRule         |                      重试机制的选择逻辑                      |

通过定义IRule实现可以修改负载均衡规则，有两种方式

1.代码方式：在order-service中的OrderApplication类中，定义一个新的IRule

```java
@Bean
public IRule randomRule() {
    return new RandomRule();
}
```

2.配置文件方式：在order-service的application.yml文件中，添加新的配置也可以修改规则

```yaml
userservice:
  ribbon:
    NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule
```

### 饥饿加载

Ribbon默认是采用懒加载，即第一次访问时才会去创建LoadBalanceClient，请求时间会很长。

而饥饿加载则会在项目启动时创建，降低第一次访问的耗时，通过下面配置开启饥饿加载。

```yaml
ribbon:
  eager-load:
    enabled: true   #开启饥饿加载
    clients:    #指定饥饿加载的服务名称
      - userservice
```

## Nacos注册中心

### 服务注册到Nacos

1.在cloud-demo父工程中添加spring-cloud-alilbaba的管理依赖

```xml
<!--nacos的管理依赖-->
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-alibaba-dependencies</artifactId>
    <version>2.2.5.RELEASE</version>
    <type>pom</type>
    <scope>import</scope>
</dependency>
```

2.注释掉order-service和user-service中原有的eureka依赖

3.添加nacos的客户端依赖

```xml
<!--nacos客户端依赖包-->
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
</dependency>
```

4.修改user-service&order-service中的application.yml文件，注释eureka地址，添加nacos地址

```yaml
spring:
  cloud:
    nacos:
      server-addr: localhost:8848   #nacos服务地址
```

### 服务跨集群调用问题

服务调用尽可能选择本地集群的服务，跨集群调用延迟较高。

本地集群不可访问时，再去访问其它集群。

**服务集群属性**

Nacos服务分级存储模型

- 一级是服务，例如userservice
- 二级是集群，例如杭州或上海
- 三级是实例，例如杭州机房的某台部署了userservice的服务器

设置实例的集群属性

- 修改application.yml文件，添加spring.cloud.nacos.discovery.cluster-name属性即可

```yaml
spring:
  cloud:
    nacos:
      server-addr: localhost:8848   #nacos服务地址
      discovery:
        cluster-name: SH    #集群名称，这里HZ代指杭州
```

根据集群负载均衡

1.修改order-service中的application.yml，设置集群为HZ

```yaml
spring:
  cloud:
    nacos:
      server-addr: localhost:8848
      discovery:
        cluster-name: HZ    #集群名称
```

2.然后在order-service中设置负载均衡的IRule为NacosRule，这个规则优先会寻找与自己同集群的服务

```yaml
userservice:
  ribbon:
    NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule   #负载均衡规则
```

NacosRule负载均衡策略

- 优先选择同集群服务实例列表
- 本地集群找不到提供者，才去其它集群寻找，并且会报警告
- 确定了可用实例列表后，再采用随机负载均衡挑选实例

### 根据权重负载均衡

实际部署中会出现这样的场景：服务器设备性能有差异，部分实例所在机器性能较好，另一些较差，我们希望性能好的机器承担更多的用户请求。

Nacos提供了权重配置来控制访问频率，权重越大则访问频率越高。

实例的权重控制

- Nacos控制台可以设置实例的权重值，0~1之间
- 同集群内的多个实例，权重越高被访问的频率越高
- 权重设置为0则完全不会被访问

### 环境隔离 - namespace

Nacos中服务存储和数据存储的最外层都是一个名为namespace的东西，用来做最外层隔离。

隔离步骤

1. 在Nacos控制台可以创建namespace，用来隔离不同环境
2. 然后填写一个新的命名空间信息，如dev开发环境
3. 保存后会在控制台看到这个命名空间的id
4. 修改order-service的application.yml，添加namespace

```yaml
spring:
  datasource:
    url: jdbc:mysql://localhost:3306/cloud_order?useSSL=false
    username: root
    password: root
    driver-class-name: com.mysql.jdbc.Driver
  application:
    name: orderservice
  cloud:
    nacos:
      server-addr: localhost:8848
      discovery:
        cluster-name: HZ    #集群名称
        namespace: 5453ee12-7884-4342-b8ee-1f51d33aebf6   #dev环境
```

5. 重启order-service后，再来查看控制台
6. 此时访问order-service，因为namespace不同，会导致找不到userservice，控制台会报错

小结

- namespace用来做环境隔离
- 每个namespace都有唯一id
- 不同namespace下的服务不可见

### 临时实例和非临时实例

服务注册到Nacos时，可以选择注册为临时或非临时实例，通过下面的配置来设置

```yaml
spring:
  cloud:
    nacos:
      server-addr: localhost:8848
      discovery:
        cluster-name: HZ    #集群名称
        namespace: 5453ee12-7884-4342-b8ee-1f51d33aebf6   #dev环境
        ephemeral: false    #是否是临时实例
```

临时实例宕机时，会从nacos的服务列表中剔除，而非临时实例则不会

### Nacos与Eureka的对比

Nacos与Eureka的共同点

- 都支持服务注册和服务拉取
- 都支持服务提供者心跳方式做健康检测

Nacos与Eureka的区别

- Nacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式
- 临时实例心跳不正常会被剔除，非临时实例则不会被剔除
- Nacos支持服务列表变更的消息推送模式，服务列表更新更及时
- Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式

## Nacos配置管理

### 统一配置管理

配置更改热更新

- 在Nacos配置列表中添加配置信息
- 在弹出表单中填写配置信息
- 引入Nacos的配置管理客户端依赖

```xml
<!--Nacos的配置管理依赖-->
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
</dependency>
```

- 在userservice中的resource目录添加一个bootstrap.yml文件，这个文件是引导文件，优先级高于application.yml

```yaml
spring:
  application:
    name: userservice   # 服务名称
  profiles:
    active: dev   # 环境
  cloud:
    nacos:
      server-addr: localhost:8848   # nacos地址
      config:
        file-extension: yaml  # 文件后缀名
```

- 在user-service中将pattern.dateformat这个属性注入到UserController中做测试

```java
@Slf4j
@RestController
@RequestMapping("/user")
public class UserController {
    @Value("${pattern.dateformat}")
    private String dateformat;

    @GetMapping("now")
    public String now() {
        return LocalDateTime.now().format(DateTimeFormatter.ofPattern(dateformat));
    }
}
```

将配置交给Nacos管理的步骤

1. 在Nacos中添加配置文件
2. 在微服务中引入nacos的config依赖
3. 在微服务中添加bootstrap.yml，配置nacos地址、当前环境、服务名称、文件后缀名。这些决定了程序启动时去nacos读取哪个文件

### 配置自动刷新

Nacos中的配置文件变更后，微服务无需重启就可以感知。不过需要通过下面两种配置实现：

方式一：在@Value注入的变量所在类上添加注解@RefreshScope

```java
@Slf4j
@RestController
@RequestMapping("/user")
@RefreshScope
public class UserController {
    @Value("${pattern.dateformat}")
    private String dateformat;

    @GetMapping("now")
    public String now() {
        return LocalDateTime.now().format(DateTimeFormatter.ofPattern(dateformat));
    }
}
```

方式二：使用@ConfigurationProperties注解

```java
@Data
@Component
@ConfigurationProperties(prefix = "pattern")
public class PatternProperties {
    private String dateformat;
}
```

总结

Nacos配置更改后，微服务可以实现热更新，方式

1. 通过@Value注解注入，结合@RefreshScope来刷新
2. 通过@ConfigurationProperties注入，自动刷新

注意事项

- 不是所有的配置都适合放到配置中心，维护起来比较麻烦
- 建议将一些关键参数，需要运行时调整的参数放到nacos配置中心，一般都是自定义配置

### 多环境配置共享

微服务启动时会从nacos读取多个配置文件

- [spring.application.name]-[spring.profiles.active].yaml，例如：userservice-dev.yaml

- [spring.application.name].yaml，例如：userservice.yaml

无论profile如何变化，[spring.application.name].yaml这个文件一定会加载，因此多环境共享配置可以写入这个文件

多种配置的优先级

- 服务名-profile.yaml > 服务名称.yaml > 本地配置

### Nacos集群搭建

搭建集群的基本步骤

- 搭建数据库，初始化数据库表结构
- 下载nacos安装包
- 配置nacos，修改集群配置（节点信息）、数据库配置
- 启动nacos集群
- nginx反向代理

## http客户端Feign

### Feign替代RestTemplate

RestTemplate方式调用存在的问题

- 代码可读性差，编程体验不统一
- 参数复杂URL难以维护

Feign的介绍

Feign是一个声明式的http客户端，官方地址：https://github.com/OpenFeign/feign

其作用就是帮助我们优雅的实现http请求的发送，解决上面提到的问题。

Feign的使用步骤

1. 引入依赖

```xml
<!--feign客户端依赖-->
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
```

2. 在order-service的启动类添加注解开启Feign的功能

```java
@MapperScan("cn.itcast.order.mapper")
@SpringBootApplication
@EnableFeignClients
public class OrderApplication {
    public static void main(String[] args) {
        SpringApplication.run(OrderApplication.class, args);
    }
}
```

3. 编写Feign客户端

```java
@FeignClient("userservice")
public interface UserClient {
    @GetMapping("/user/{id}")
    User findById(@PathVariable("id") Long id);
}
```

主要是基于SpringMVC的注解来声明远程调用的信息

- 服务名称：userservice

- 请求方式：GET

- 请求路径：/user/{id}

- 请求参数：Long id

- 返回值类型：User

4. 用Feign客户端代替RestTemplate

```java
@Service
public class OrderService {
    @Autowired
    private OrderMapper orderMapper;

    @Autowired
    private UserClient userClient;

    public Order queryOrderById(Long orderId) {
        Order order = orderMapper.findById(orderId);
        User user = userClient.findById(order.getUserId());
        order.setUser(user);
        return order;
    }
}
```

### 自定义Feign的配置

Feign运行自定义配置来覆盖默认配置，可以修改的配置如下：

|        类型         |       作用       |                          说明                          |
| :-----------------: | :--------------: | :----------------------------------------------------: |
| feign.Logger.Level  |   修改日志级别   |     包含四种不同的级别：NONE、BASIC、HEADERS、FULL     |
| feign.codec.Decoder | 响应结果的解析器 | http远程调用的结果做解析，例如解析json字符串为java对象 |
| feign.codec.Encoder |   请求参数编码   |          将请求参数编码，便于通过http请求发送          |
|   feign. Contract   |  支持的注解格式  |                 默认是SpringMVC的注解                  |
|   feign. Retryer    |   失败重试机制   | 请求失败的重试机制，默认是没有，不过会使用Ribbon的重试 |

一般我们需要配置的就是日志级别。

配置Feign日志有两种方式

方式一：配置文件方式

- 全局生效

```yaml
feign:
	client:
		config: 
			default: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置
				loggerLevel: FULL #  日志级别 
```

- 局部生效

```yaml
feign:
	client:
		config: 
			userservice: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置
				loggerLevel: FULL #  日志级别 
```

配置Feign日志的方式二：java代码方式，需要先声明一个Bean

```java
public class DefaultFeignConfiguration {
    @Bean
    public Logger.Level logLevel() {
        return Logger.Level.BASIC;
    }
}
```

- 如果是全局配置，则把它放到@EnableFeignClients这个注解中

```java
@EnableFeignClients(defaultConfiguration = DefaultFeignConfiguration.class)
```

- 如果是局部配置，则把它放到@FeignClient这个注解中

```java
@FeignClient(value = "userservice", configuration = FeignClientConfiguration.class)
```

### Feign的性能优化

Feign底层的客户端实现

- URLConnection：默认实现，不支持连接池

- Apache HttpClient：支持连接池

- OKHttp：支持连接池

因此优化Feign的性能主要包括

- 使用连接池代替默认的URLConnection

- 日志级别，最好用basic或none

连接池配置(Feign添加HttpClient的支持)

引入依赖

```java
<!--引入HTTPClient依赖-->
<dependency>
    <groupId>io.github.openfeign</groupId>
    <artifactId>feign-httpclient</artifactId>
</dependency>
```

配置连接池

```yaml
feign:
  httpclient:
    enabled: true   # 支持HttpClient的开关
    max-connections: 200    # 最大连接数
    max-connections-per-route: 50   # 单个路径的最大连接数
```

### Feign的最佳实践

方式一(继承)：给消费者的FeignClient和提供者的controller定义统一的父接口作为标准。

- 服务紧耦合
- 父接口参数列表中的映射不会被继承

方式二(抽取)：将FeignClient抽取为独立模块，并且把接口有关的POJO、默认的Feign配置都放到这个模块中，提供给所有消费者使用。

实现最佳实践方式二的步骤

1. 首先创建一个module，命名为feign-api，然后引入feign的starter依赖
2. 将order-service中编写的UserClient、User、DefaultFeignConfiguration都复制到feign-api项目中
3. 在order-service中引入feign-api的依赖
4. 修改order-service中的所有与上述三个组件有关的import部分，改成导入feign-api中的包
5. 重启测试

当定义的FeignClient不在SpringBootApplication的扫描包范围时，这些FeignClient无法使用。有两种方式解决

- 在@EnableFeignClients注解中添加basePackages，指定FeignClient所在的包

```java
@EnableFeignClients(basePackages = "cn.itcast.feign.clients")
```

- 在@EnableFeignClients注解中添加clients，指定具体FeignClient的字节码

```java
@EnableFeignClients(clients = {UserClient.class})
```

## 统一网关Gateway

### 为什么需要网关

网关功能

- 身份认证和权限校验

- 服务路由、负载均衡

- 请求限流

在SpringCloud中网关的实现包括两种

- gateway

- zuul

Zuul是基于Servlet的实现，属于阻塞式编程。而SpringCloudGateway则是基于Spring5中提供的WebFlux，属于响应式编程的实现，具备更好的性能。

### 搭建网关服务

搭建网关服务的步骤

1. 创建新的module，引入SpringCloudGateway的依赖和nacos的服务发现依赖

```xml
<!--nacos服务注册发现依赖-->
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
</dependency>
<!--网关gateway依赖-->
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-gateway</artifactId>
</dependency>
```

2. 编写路由配置及nacos地址

```yaml
server:
  port: 10010
spring:
  application:
    name: gateway
  cloud:
    nacos:
      server-addr: localhost:8848  # nacos地址
    gateway:
      routes:
        - id: user-service  # 路由标识，必须唯一
          uri: lb://userservice  # 路由的目标地址
          predicates:   # 路由断言，判断请求是否符合规则
            - Path=/user/**  # 路径断言，判断路径是否是以/user开头，如果是则符合
        - id: order-service
          uri: lb://orderservice
          predicates:
            - Path=/order/**
```

路由配置项

1. 路由id：路由的唯一标示

2. 路由目标（uri）：路由的目标地址，http代表固定地址，lb代表根据服务名负载均衡

3. 路由断言（predicates）：判断路由的规则，

4. 路由过滤器（filters）：对请求或响应做处理

### 路由断言工厂Route Predicate Factory

我们在配置文件中写的断言规则只是字符串，这些字符串会被Predicate Factory读取并处理，转变为路由判断的条件

例如Path=/user/**是按照路径匹配，这个规则是由org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory类来处理的

Spring提供了11种基本的Predicate工厂

|  **名称**  |            **说明**            |                           **示例**                           |
| :--------: | :----------------------------: | :----------------------------------------------------------: |
|   After    |      是某个时间点后的请求      |    -  After=2037-01-20T17:42:47.789-07:00[America/Denver]    |
|   Before   |     是某个时间点之前的请求     |    -  Before=2031-04-13T15:14:47.433+08:00[Asia/Shanghai]    |
|  Between   |    是某两个时间点之前的请求    | -  Between=2037-01-20T17:42:47.789-07:00[America/Denver],  2037-01-21T17:42:47.789-07:00[America/Denver] |
|   Cookie   |     请求必须包含某些cookie     |                   - Cookie=chocolate, ch.p                   |
|   Header   |     请求必须包含某些header     |                  - Header=X-Request-Id, \d+                  |
|    Host    | 请求必须是访问某个host（域名） |         -  Host=\**.somehost.org,**.anotherhost.org          |
|   Method   |     请求方式必须是指定方式     |                      - Method=GET,POST                       |
|    Path    |    请求路径必须符合指定规则    |                - Path=/red/{segment},/blue/**                |
|   Query    |    请求参数必须包含指定参数    |             - Query=name, Jack或者-  Query=name              |
| RemoteAddr |    请求者的ip必须是指定范围    |                 - RemoteAddr=192.168.1.1/24                  |
|   Weight   |            权重处理            |                                                              |

### 路由过滤器GatewayFilter

GatewayFilter是网关中提供的一种过滤器，可以对进入网关的请求和微服务返回的响应做处理。

Spring提供了31种不同的路由过滤器工厂

|       **名称**       |           **说明**           |
| :------------------: | :--------------------------: |
|   AddRequestHeader   |   给当前请求添加一个请求头   |
| RemoveRequestHeader  |    移除请求中的一个请求头    |
|  AddResponseHeader   |  给响应结果中添加一个响应头  |
| RemoveResponseHeader | 从响应结果中移除有一个响应头 |
|  RequestRateLimiter  |        限制请求的流量        |
|         ...          |                              |

给所有进入userservice的请求添加一个请求头

```yaml
spring:
  application:
    name: gateway
  cloud:
    nacos:
      server-addr: localhost:8848  # nacos地址
    gateway:
      routes:
        - id: user-service  # 路由标识，必须唯一
          uri: lb://userservice  # 路由的目标地址
          predicates:   # 路由断言，判断请求是否符合规则
            - Path=/user/**  # 路径断言，判断路径是否是以/user开头，如果是则符合
          filters:  # 过滤器
            - AddRequestHeader=Truth,Mark is freaking aowsome!  # 添加请求
        - id: order-service
          uri: lb://orderservice
          predicates:
            - Path=/order/**
            - Before=2031-04-13T15:14:47.433+08:00[Asia/Shanghai]
```

默认过滤器：如果要对所有的路由都生效，则可以将过滤器工厂写到default下。

```yaml
spring:
  application:
    name: gateway
  cloud:
    nacos:
      server-addr: localhost:8848  # nacos地址
    gateway:
      routes:
        - id: user-service  # 路由标识，必须唯一
          uri: lb://userservice  # 路由的目标地址
          predicates:   # 路由断言，判断请求是否符合规则
            - Path=/user/**  # 路径断言，判断路径是否是以/user开头，如果是则符合
#          filters:  # 过滤器
#            - AddRequestHeader=Truth,Mark is freaking aowsome!  # 添加请求
        - id: order-service
          uri: lb://orderservice
          predicates:
            - Path=/order/**
            - Before=2031-04-13T15:14:47.433+08:00[Asia/Shanghai]
      default-filters:  # 默认过滤器，会对所有的路由请求都生效
        - AddRequestHeader=Truth,Mark is freaking aowsome!  # 添加请求
```

### 全局过滤器GlobalFilter

全局过滤器的作用也是处理一切进入网关的请求和微服务响应，与GatewayFilter的作用一样。

区别在于GatewayFilter通过配置定义，处理逻辑是固定的。而GlobalFilter的逻辑需要自己写代码实现。

定义方式是实现GlobalFilter接口。

```java
public interface GlobalFilter {
    /**
    *  处理当前请求，有必要的话通过{@link GatewayFilterChain}将请求交给下一个过滤器处理
    *
    * @param exchange 请求上下文，里面可以获取Request、Response等信息
    * @param chain 用来把请求委托给下一个过滤器 
    * @return {@code Mono<Void>} 返回标示当前过滤器业务结束
    */
    Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain);
}
```

自定义类，实现GlobalFilter接口，添加@Order注解或实现Ordered接口

```java
//@Order(-1)
@Component
public class AuthorizeFilter implements GlobalFilter, Ordered {
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        // 1.获取请求参数
        ServerHttpRequest request = exchange.getRequest();
        // 2.获取参数中的authorization参数
        MultiValueMap<String, String> params = request.getQueryParams();
        // 3.判断参数值是否等于admin
        String auth = params.getFirst("authorization");
        if ("admin".equals(auth)) {
            // 4.是，放行
            return chain.filter(exchange);
        }
        // 5.否，拦截
        // 5.1.设置状态码
        exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED);
        // 5.2.拦截请求
        return exchange.getResponse().setComplete();
    }

    @Override
    public int getOrder() {
        return -1;
    }
}
```

### 过滤器执行顺序

请求进入网关会碰到三类过滤器：当前路由的过滤器、DefaultFilter、GlobalFilter

请求路由后，会将当前路由过滤器和DefaultFilter、GlobalFilter，合并到一个过滤器链（集合）中，排序后依次执行每个过滤器

- 每一个过滤器都必须指定一个int类型的order值，order值越小，优先级越高，执行顺序越靠前

- GlobalFilter通过实现Ordered接口，或者添加@Order注解来指定order值，由我们自己指定

- 路由过滤器和defaultFilter的order由Spring指定，默认是按照声明顺序从1递增

- 当过滤器的order值一样时，会按照 defaultFilter > 路由过滤器 > GlobalFilter的顺序执行

### 跨域问题处理

跨域：域名不一致就是跨域，主要包括：

- 域名不同： www.taobao.com 和 www.taobao.org 和 www.jd.com 和 miaosha.jd.com

- 域名相同，端口不同：localhost:8080和localhost8081

跨域问题：浏览器禁止请求的发起者与服务端发生跨域ajax请求，请求被浏览器拦截的问题

解决方案：CORS

网关处理跨域采用的同样是CORS方案，并且只需要简单配置即可实现

```yaml
spring:
  cloud:
    gateway:
      globalcors: # 全局的跨域处理
        add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题
        corsConfigurations:
          '[/**]':
            allowedOrigins: # 允许哪些网站的跨域请求
              - "http://localhost:8090"
              - "http://www.leyou.com"
            allowedMethods: # 允许的跨域ajax的请求方式
              - "GET"
              - "POST"
              - "DELETE"
              - "PUT"
              - "OPTIONS"
            allowedHeaders: "*" # 允许在请求中携带的头信息
            allowCredentials: true # 是否允许携带cookie
            maxAge: 360000 # 这次跨域检测的有效期
```

## 服务异步通讯

### 初识MQ

**同步调用**

同步调用的优点

- 时效性较强，可以立即得到结果

同步调用存在的问题

- 耦合度高：每次加入新的需求，都要修改原来的代码
- 性能下降：调用者需要等待服务提供者响应，如果调用链过长则响应时间等于每次调用的时间之和
- 资源浪费：调用链中的每个服务在等待响应过程中，不能释放请求占用的资源，高并发场景下会极度浪费系统资源
- 级联失败：如果服务提供者出现问题，所有调用方都会跟着出问题，如同多米诺骨牌一样，迅速导致整个微服务群故障

**异步调用**

异步通信的优点

- 耦合度低
- 吞吐量提升
- 故障隔离
- 流量削峰

异步通信的缺点

- 依赖于Broker的可靠性、安全性、吞吐能力
- 架构复杂了，业务没有明显的流程线，不好追踪管理

**MQ概念**

MQ(MessageQueue)，中文是消息队列，字面来看就是存放消息的队列。也就是事件驱动架构中的Broker。

|            |        RabbitMQ         |             ActiveMQ              |  RocketMQ  |   Kafka    |
| :--------: | :---------------------: | :-------------------------------: | :--------: | :--------: |
| 公司/社区  |         Rabbit          |              Apache               |    阿里    |   Apache   |
|  开发语言  |         Erlang          |               Java                |    Java    | Scala&Java |
|  协议支持  | AMQP，XMPP，SMTP，STOMP | OpenWire，STOMP，REST，XMPP，AMQP | 自定义协议 | 自定义协议 |
|   可用性   |           高            |               一般                |     高     |     高     |
| 单机吞吐量 |          一般           |                差                 |     高     |   非常高   |
|  消息延迟  |         微秒级          |              毫秒级               |   毫秒级   |  毫秒以内  |
| 消息可靠性 |           高            |               一般                |     高     |    一般    |

RabbitMQ中的几个概念

- channel：操作MQ的工具
- exchange：路由消息到队列中
- queue：缓存消息
- virtual host：虚拟主机，是对queue、exchange等资源的逻辑分组

**常见消息模型**

MQ的官方文档中给出了5个MQ的Demo示例，对应了几种不同的用法：

- 基本消息队列（BasicQueue）
- 工作消息队列（WorkQueue）
- 发布订阅（Publish、Subscribe），又根据交换机类型不同分为三种：
  - Fanout Exchange：广播
  - Direct Exchange：路由
  - Topic Exchange：主题

**消息队列处理流程**

基本消息队列的消息发送流程

1. 建立connection

2. 创建channel

3. 利用channel声明队列

4. 利用channel向队列发送消息

基本消息队列的消息接收流程

1. 建立connection

2. 创建channel

3. 利用channel声明队列

4. 定义consumer的消费行为handleDelivery()

5. 利用channel将消费者与队列绑定

### SpringAMQP

**基础概念**

SpringAmqp的官方地址：https://spring.io/projects/spring-amqp

Advanced Message Queuing Protocol，是用于在应用程序之间传递业务消息的开放标准。该协议与语言和平台无关，更符合微服务中独立性的要求。

Spring AMQP是基于AMQP协议定义的一套API规范，提供了模板来发送和接收消息。包含两部分，其中spring-amqp是基础抽象，spring-rabbit是底层的默认实现。

**SpringAMQP发送消息**

- 引入amqp的starter依赖
- 配置RabbitMQ地址
- 利用RabbitTemplate的convertAndSend方法

**SpringAMQP接收消息**

- 引入amqp的starter依赖
- 配置RabbitMQ地址
- 定义类，添加@Component注解
- 类中声明方法，添加@RabbitListener注解，方法参数就是消息

注意：消息一旦消费就会从队列删除，RabbitMQ没有消息回溯功能。

**Work Queue工作队列**

Work queue，工作队列，可以提高消息处理速度，避免队列消息堆积。

Work模型的使用

- 多个消费者绑定到一个队列，同一条消息只会被一个消费者处理

- 通过设置prefetch来控制消费者预取的消息数量

**发布(Publish)、订阅(Subscribe)**

发布订阅模式与之前案例的区别就是允许将同一消息发送给多个消费者。实现方式是加入了exchange（交换机）。

常见exchange类型包括：

- Fanout：广播

- Direct：路由

- Topic：话题

注意：exchange负责消息路由，而不是存储，路由失败则消息丢失。

**发布订阅-FanoutExchange**

Fanout Exchange会将接收到的消息广播到每一个跟其绑定的queue。

交换机的作用

- 接收publisher发送的消息

- 将消息按照规则路由到与之绑定的队列

- 不能缓存消息，路由失败，消息丢失

- FanoutExchange会将消息路由到每个绑定的队列

声明队列、交换机、绑定关系的Bean是什么？

- Queue

- FanoutExchange

- Binding

**发布订阅-DirectExchange**

Direct Exchange会将接收到的消息根据规则路由到指定的Queue，因此称为路由模式（routes）。

- 每一个Queue都与Exchange设置一个BindingKey

- 发布者发送消息时，指定消息的RoutingKey

- Exchange将消息路由到BindingKey与消息RoutingKey一致的队列

Direct交换机与Fanout交换机的差异

- Fanout交换机将消息路由给每一个与之绑定的队列

- Direct交换机根据RoutingKey判断路由给哪个队列

- 如果多个队列具有相同的RoutingKey，则与Fanout功能类似

基于@RabbitListener注解声明队列和交换机常见注解

- @Queue

- @Exchange

**发布订阅-TopicExchange**

TopicExchange与DirectExchange类似，区别在于routingKey必须是多个单词的列表，并且以 **.** 分割。

Queue与Exchange指定BindingKey时可以使用通配符：

- #：代指0个或多个单词

- *：代指一个单词

Direct交换机与Topic交换机的差异

- Topic交换机接收的消息RoutingKey必须是多个单词，以 **.** 分割

- Topic交换机与队列绑定时的bindingKey可以指定通配符

- #：代表0个或多个词

- *：代表1个词

**消息转换器**

Spring的对消息对象的处理是由org.springframework.amqp.support.converter.MessageConverter来处理的。而默认实现是SimpleMessageConverter，基于JDK的ObjectOutputStream完成序列化。

如果要修改只需要定义一个MessageConverter 类型的Bean即可。推荐用JSON方式序列化。

SpringAMQP中消息的序列化和反序列化

- 利用MessageConverter实现的，默认是JDK的序列化

- 注意发送方与接收方必须使用相同的MessageConverter

## 微服务保护

### 初识Sentinel

**雪崩问题**

微服务调用链路中的某个服务故障，引起整个链路中的所有微服务都不可用，这就是雪崩。

解决雪崩问题的四种常见方式

- 超时处理：设定超时时间，请求超过一定时间没有响应就返回错误信息，不会无休止等待
- 舱壁模式：限定每个业务能使用的线程数，避免耗尽整个tomcat的资源，因此也叫线程隔离
- 熔断降级：由断路器统计业务执行的异常比例，如果超出阈值则会熔断该业务，拦截访问该业务的一切请求
- 流量控制：限制业务访问的QPS，避免服务因流量的突增而故障(预防)

**服务保护技术对比**

|                |                    Sentinel                    |            Hystrix            |
| :------------: | :--------------------------------------------: | :---------------------------: |
|    隔离策略    |                   信号量隔离                   |     线程池隔离/信号量隔离     |
|  熔断降级策略  |            基于慢调用比例或异常比例            |         基于失败比率          |
|  实时指标实现  |                    滑动窗口                    |    滑动窗口（基于 RxJava）    |
|    规则配置    |                 支持多种数据源                 |        支持多种数据源         |
|     扩展性     |                   多个扩展点                   |          插件的形式           |
| 基于注解的支持 |                      支持                      |             支持              |
|      限流      |        基于 QPS，支持基于调用关系的限流        |          有限的支持           |
|    流量整形    |            支持慢启动、匀速排队模式            |            不支持             |
| 系统自适应保护 |                      支持                      |            不支持             |
|     控制台     | 开箱即用，可配置规则、查看秒级监控、机器发现等 |            不完善             |
| 常见框架的适配 |     Servlet、Spring Cloud、Dubbo、gRPC  等     | Servlet、Spring Cloud Netflix |

**认识Sentinel**

Sentinel是阿里巴巴开源的一款微服务流量控制组件。官网地址：https://sentinelguard.io/zh-cn/index.html

Sentinel具有的特征

- 丰富的应用场景：Sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、集群流量控制、实时熔断下游不可用应用等。

- 完备的实时监控：Sentinel 同时提供实时的监控功能。您可以在控制台中看到接入应用的单台机器秒级数据，甚至 500 台以下规模的集群的汇总运行情况。
- 广泛的开源生态：Sentinel 提供开箱即用的与其它开源框架/库的整合模块，例如与 Spring Cloud、Dubbo、gRPC 的整合。您只需要引入相应的依赖并进行简单的配置即可快速地接入 Sentinel。
- 完善的SPI扩展点：Sentinel 提供简单易用、完善的 SPI 扩展接口。您可以通过实现扩展接口来快速地定制逻辑。例如定制规则管理、适配动态数据源等。

Sentinel控制台

如果要修改Sentinel的默认端口、账户、密码，可以通过下列配置

|              配置项              |  默认值  |    说明    |
| :------------------------------: | :------: | :--------: |
|           server.port            |   8080   |  服务端口  |
| sentinel.dashboard.auth.username | sentinel | 默认用户名 |
| sentinel.dashboard.auth.password | sentinel |  默认密码  |

修改示例

```shell
java -jar sentinel-dashboard-1.8.1.jar -Dserver.port=8090
```

### 限流规则

**簇点链路**

簇点链路：就是项目内的调用链路，链路中被监控的每个接口就是一个资源。默认情况下sentinel会监控SpringMVC的每一个端点（Endpoint），因此SpringMVC的每一个端点（Endpoint）就是调用链路中的一个资源。

**流控模式**

在添加限流规则时，点击高级选项，可以选择三种流控模式

- 直接：统计当前资源的请求，触发阈值时对当前资源直接限流，也是默认的模式
- 关联：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流
- 链路：统计从指定链路访问到本资源的请求，触发阈值时，对指定链路限流

流控模式-关联

- 关联模式：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流
- 使用场景：比如用户支付时需要修改订单状态，同时用户要查询订单。查询和修改操作会争抢数据库锁，产生竞争。业务需求是有限支付和更新订单的业务，因此当修改订单业务触发阈值时，需要对查询订单业务限流

流控模式-链路

- 链路模式：只针对从指定链路访问到本资源的请求做统计，判断是否超过阈值

- 有查询订单和创建订单业务，两者都需要查询商品。针对从查询订单进入到查询商品的请求统计，并设置限流

**流控效果**

流控效果是指请求达到流控阈值时应该采取的措施，包括三种：

- 快速失败：达到阈值后，新的请求会被立即拒绝并抛出FlowException异常。是默认的处理方式
- warm up：预热模式，对超出阈值的请求同样是拒绝并抛出异常。但这种模式阈值会动态变化，从一个较小值逐渐增加到最大阈值
- 排队等待：让所有的请求按照先后次序排队执行，两个请求的间隔不能小于指定时长

**热点参数限流**

之前的限流是统计访问某个资源的所有请求，判断是否超过QPS阈值。而热点参数限流是分别统计参数值相同的请求，判断是否超过QPS阈值

注意：热点参数限流对默认的SpringMVC资源无效

### 隔离和降级

**隔离和降级**

虽然限流可以尽量避免因高并发而引起的服务故障，但服务还会因为其它原因而故障。而要将这些故障控制在一定范围，避免雪崩，就要靠线程隔离（舱壁模式）和熔断降级手段了。

不管是线程隔离还是熔断降级，都是对客户端（调用方）的保护。

**线程隔离**

线程隔离有两种方式实现

- 线程池隔离
- 信号量隔离（Sentinel默认采用）

线程池隔离

- 优点
  - 支持主动超时
  - 支持异步调用
- 缺点
  - 线程的额外开销比较大
- 场景
  - 低扇出

信号量隔离

- 优点
  - 轻量级，无额外开销
- 缺点
  - 不支持主动超时
  - 不支持异步调用
- 场景
  - 高频调用
  - 高扇出

**熔断降级**

熔断降级是解决雪崩问题的重要手段。其思路是由**断路器**统计服务调用的异常比例、慢请求比例，如果超出阈值则会**熔断**该服务。即拦截访问该服务的一切请求；而当服务恢复时，断路器会放行访问该服务的请求。

断路器熔断策略有三种：慢调用、异常比例、异常数

熔断策略-慢调用

- 慢调用：业务的响应时长（RT）大于指定时长的请求认定为慢调用请求。在指定时间内，如果请求数量超过设定的最小数量，慢调用比例大于设定的阈值，则触发熔断
- 异常比例或异常数：统计指定时间内的调用，如果调用次数超过指定请求数，并且出现异常的比例达到设定的比例阈值（或超过指定异常数），则触发熔断

### 授权规则

授权规则可以对调用方的来源做控制，有白名单和黑名单两种方式

- 白名单：来源（origin）在白名单内的调用者允许访问
- 黑名单：来源（origin）在黑名单内的调用者不允许访问

Sentinel是通过RequestOriginParser这个接口的parseOrigin来获取请求的来源的。

**自定义异常结果**

默认情况下，发生限流、降级、授权拦截时，都会抛出异常到调用方。如果要自定义异常时的返回结果，需要实现BlockExceptionHandler接口。

BlockException包含很多个子类，分别对应不同的场景

|         异常         |        说明        |
| :------------------: | :----------------: |
|    FlowException     |      限流异常      |
|  ParamFlowException  | 热点参数限流的异常 |
|   DegradeException   |      降级异常      |
|  AuthorityException  |    授权规则异常    |
| SystemBlockException |    系统规则异常    |

### 规则持久化

**规则管理模式**

Sentinel的控制台规则管理有三种模式

|                           推送模式                           |                             说明                             |             优点             |                             缺点                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :--------------------------: | :----------------------------------------------------------: |
| [原始模式](https://github.com/alibaba/Sentinel/wiki/在生产环境中使用-Sentinel) | API 将规则推送至客户端并直接更新到内存中，扩展写数据源（[WritableDataSource](https://github.com/alibaba/Sentinel/wiki/动态规则扩展)），默认就是这种 |       简单，无任何依赖       | 不保证一致性；规则保存在内存中，重启即消失。严重不建议用于生产环境 |
| [Pull](https://github.com/alibaba/Sentinel/wiki/在生产环境中使用-Sentinel)[模式](https://github.com/alibaba/Sentinel/wiki/在生产环境中使用-Sentinel) | 扩展写数据源（[WritableDataSource](https://github.com/alibaba/Sentinel/wiki/动态规则扩展)），  客户端主动向某个规则管理中心定期轮询拉取规则，这个规则中心可以是 RDBMS、文件 等 | 简单，无任何依赖；规则持久化 | 不保证一致性；实时性不保证，拉取过于频繁也可能会有性能问题。 |
| [Push](https://github.com/alibaba/Sentinel/wiki/在生产环境中使用-Sentinel)[模式](https://github.com/alibaba/Sentinel/wiki/在生产环境中使用-Sentinel) | 扩展读数据源（[ReadableDataSource](https://github.com/alibaba/Sentinel/wiki/动态规则扩展)），规则中心统一推送，客户端通过注册监听器的方式时刻监听变化，比如使用  Nacos、Zookeeper  等配置中心。这种方式有更好的实时性和一致性保证。**生产环境下一般采用**  **push**  **模式的数据源。** |     规则持久化；一致性；     |                        引入第三方依赖                        |

原始模式：控制台配置的规则直接推送到Sentinel客户端，也就是我们的应用。然后保存在内存中，服务重启则丢失

Pull模式：控制台将配置的规则推送到Sentinel客户端，而客户端会将配置规则保存在本地文件或数据库中。以后会定时去本地文件或数据库中查询，更新本地规则

Push模式：控制台将配置规则推送到远程配置中心，例如Nacos。Sentinel客户端监听Nacos，获取配置变更的推送消息，完成本地配置更新

## 分布式事务

### 分布式事务问题

**事务的ACID原则**

原子性：事务中的所有操作，要么全部成功，要么全部失败

一致性：要保证数据库内部完整性约束、声明性约束

隔离性：对同一资源操作的事务不能同时发生

持久性：对数据库做的一切修改将永久保存，不管是否出现故障

**分布式服务的事务问题**

在分布式系统下，一个业务跨越多个服务或数据源，每个服务都是一个分支事务，要保证所有分支事务最终状态一致，这样的事务就是分布式事务。

### 理论基础

**CAP定理**

1998年，加州大学的计算机科学家 Eric Brewer 提出，分布式系统有三个指标

- Consistency（一致性）
- Availability（可用性）
- Partition tolerance （分区容错性）

Eric Brewer 说，分布式系统无法同时满足这三个指标。这个结论就叫做 CAP 定理。

Consistency（一致性）：用户访问分布式系统中的任意节点，得到的数据必须一致。

Availability （可用性）：用户访问集群中的任意健康节点，必须能得到响应，而不是超时或拒绝。

Partition（分区）：因为网络故障或其它原因导致分布式系统中的部分节点与其它节点失去连接，形成独立分区。

Tolerance（容错）：在集群出现分区时，整个系统也要持续对外提供服务。

**BASE理论**

BASE理论是对CAP的一种解决思路，包含三个思想

- **Basically Available** **（基本可用）**：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。
- **Soft State（软状态）：**在一定时间内，允许出现中间状态，比如临时的不一致状态。
- **Eventually Consistent（最终一致性）**：虽然无法保证强一致性，但是在软状态结束后，最终达到数据一致。

分布式事务最大的问题是各个子事务的一致性问题，因此可以借鉴CAP定理和BASE理论

- AP模式：各子事务分别执行和提交，允许出现结果不一致，然后采用弥补措施恢复数据即可，实现最终一致。
- CP模式：各个子事务执行后互相等待，同时提交，同时回滚，达成强一致。但事务等待过程中，处于弱可用状态。

解决分布式事务，各个子系统之间必须能感知到彼此的事务状态，才能保证状态一致，因此需要一个事务协调者来协调每一个事务的参与者（子系统事务）。

这里的子系统事务，称为分支事务；有关联的各个分支事务在一起称为全局事务。

### 初识Seata

Seata是 2019 年 1 月份蚂蚁金服和阿里巴巴共同开源的分布式事务解决方案。致力于提供高性能和简单易用的分布式事务服务，为用户打造一站式的分布式解决方案。

官网地址：http://seata.io/，其中的文档、博客中提供了大量的使用说明、源码分析。

**Seata架构**

Seata事务管理中有三个重要的角色

- **TC (Transaction Coordinator) - 事务协调者：**维护全局和分支事务的状态，协调全局事务提交或回滚。
- **TM (Transaction Manager) - 事务管理器：**定义全局事务的范围、开始全局事务、提交或回滚全局事务。
- **RM (Resource Manager) - 资源管理器：**管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。

Seata提供了四种不同的分布式事务解决方案

- XA模式：强一致性分阶段事务模式，牺牲了一定的可用性，无业务侵入
- TCC模式：最终一致的分阶段事务模式，有业务侵入
- AT模式：最终一致的分阶段事务模式，无业务侵入，也是Seata的默认模式
- SAGA模式：长事务模式，有业务侵入

**XA模式原理**

XA 规范是 X/Open 组织定义的分布式事务处理（DTP，Distributed Transaction Processing）标准，XA 规范 描述了全局的TM与局部的RM之间的接口，几乎所有主流的数据库都对 XA 规范 提供了支持。

seata的XA模式做了一些调整，但大体相似

RM一阶段的工作：

- 注册分支事务到TC

- 执行分支业务sql但不提交
- 报告执行状态到TC

TC二阶段的工作：

- TC检测各分支事务执行状态
  1. 如果都成功，通知所有RM提交事务
  2. 如果有失败，通知所有RM回滚事务

RM二阶段的工作：

- 接收TC指令，提交或回滚事务

XA模式的优点

- 事务的强一致性，满足ACID原则
- 常用数据库都支持，实现简单，并且没有代码侵入

XA模式的缺点

- 因为一阶段需要锁定数据库资源，等待二阶段结束才释放，性能较差
- 依赖关系型数据库实现事务

**AT模式原理**

AT模式同样是分阶段提交的事务模型，不过缺弥补了XA模型中资源锁定周期过长的缺陷。

阶段一RM的工作：

- 注册分支事务
- 记录undo-log（数据快照）
- 执行业务sql并提交
- 报告事务状态

阶段二提交时RM的工作：

- 删除undo-log即可

阶段二回滚时RM的工作：

- 根据undo-log恢复数据到更新前

AT模式与XA模式最大的区别

- XA模式一阶段不提交事务，锁定资源；AT模式一阶段直接提交，不锁定资源
- XA模式依赖数据库机制实现回滚；AT模式利用数据快照实现数据回滚
- XA模式强一致；AT模式最终一致

AT模式的优点

- 一阶段完成直接提交事务，释放数据库资源，性能比较好
- 利用全局锁实现读写隔离
- 没有代码侵入，框架自动完成回滚和提交

AT模式的缺点

- 两阶段之间属于软状态，属于最终一致
- 框架的快照功能会影响性能，但比XA模式要好很多

**TCC模式原理**

TCC模式与AT模式非常相似，每阶段都是独立事务，不同的是TCC通过人工编码来实现数据恢复。需要实现三个方法：

- Try：资源的检测和预留
- Confirm：完成资源操作业务；要求 Try 成功 Confirm 一定要能成功
- Cancel：预留资源释放，可以理解为try的反向操作

举例，一个扣减用户余额的业务。假设账户A原来余额是100，需要余额扣减30元。

- 阶段一（ Try ）：检查余额是否充足，如果充足则冻结金额增加30元，可用余额扣除30
- 阶段二：假如要提交（Confirm），则冻结金额扣减30
- 阶段二：如果要回滚（Cancel），则冻结金额扣减30，可用余额增加30

TCC模式的每个阶段

- Try：资源检查和预留
- Confirm：业务执行和提交
- Cancel：预留资源的释放

TCC的优点

- 一阶段完成直接提交事务，释放数据库资源，性能好
- 相比AT模型，无需生成快照，无需使用全局锁，性能最强
- 不依赖数据库事务，而是依赖补偿操作，可以用于非事务型数据库

TCC的缺点

- 有代码侵入，需要人为编写try、Confirm和Cancel接口，太麻烦
- 软状态，事务是最终一致
- 需要考虑Confirm和Cancel的失败情况，做好幂等处理

TCC的空回滚和业务悬挂

1. 当某分支事务的try阶段阻塞时，可能导致全局事务超时而触发二阶段的cancel操作。在未执行try操作时先执行了cancel操作，这时cancel不能做回滚，就是空回滚
2. 对于已经空回滚的业务，如果以后继续执行try，就永远不可能confirm或cancel，这就是业务悬挂。应当阻止执行空回滚后的try操作，避免悬挂

**SAGA模式**

SAGA模式是SEATA提供的长事务解决方案。也分为两个阶段

- 一阶段：直接提交本地事务
- 二阶段：成功则什么都不做；失败则通过编写补偿业务来回滚

SAGA模式优点

- 事务参与者可以基于事件驱动实现异步调用，吞吐高
- 一阶段直接提交事务，无锁，性能好
- 不用编写TCC中的三个阶段，实现简单

缺点

- 软状态持续时间不确定，时效性差
- 没有锁，没有事务隔离，会有脏写

**四种模式对比**

|              |               XA               |                      AT                      |                            TCC                             |                             SAGA                             |
| :----------: | :----------------------------: | :------------------------------------------: | :--------------------------------------------------------: | :----------------------------------------------------------: |
|  **一致性**  |             强一致             |                    弱一致                    |                           弱一致                           |                           最终一致                           |
|  **隔离性**  |            完全隔离            |                基于全局锁隔离                |                      基于资源预留隔离                      |                            无隔离                            |
| **代码侵入** |               无               |                      无                      |                     有，要编写三个接口                     |                  有，要编写状态机和补偿业务                  |
|   **性能**   |               差               |                      好                      |                           非常好                           |                            非常好                            |
|   **场景**   | 对一致性、隔离性有高要求的业务 | 基于关系型数据库的大多数分布式事务场景都可以 | 对性能要求较高的事务。<br />有非关系型数据库要参与的事务。 | 业务流程长、业务流程多<br />参与者包含其它公司或遗留系统服务，无法提供  TCC  模式要求的三个接口 |

## 分布式缓存

### 单点Redis的问题

**面临问题**

数据丢失问题：Redis是内存存储，服务重启可能会丢失数据

并发能力问题：单节点Redis并发能力虽然不错，但也无法满足如618这样的高并发场景

故障恢复问题：如果Redis宕机，则服务不可用，需要一种自动的故障恢复手段

存储能力问题：Redis基于内存，单节点能存储的数据量难以满足海量数据需求

**解决方法**

数据丢失问题：实现Redis数据持久化

并发能力问题：搭建主从集群，实现读写分离

故障恢复问题：利用Redis哨兵，实现健康检测和自动恢复

存储能力问题：搭建分片集群，利用插槽机制实现动态扩容

### Redis持久化

**RDB**

RDB全称Redis Database Backup file（Redis数据备份文件），也被叫做Redis数据快照。简单来说就是把内存中的所有数据都记录到磁盘中。当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。

快照文件称为RDB文件，默认是保存在当前运行目录。

Redis内部有触发RDB的机制，可以在redis.conf文件中找到相关配置

```shell
# 900秒内，如果至少有1个key被修改，则执行bgsave ， 如果是save "" 则表示禁用RDB
save 900 1  
save 300 10  
save 60 10000 

# 是否压缩 ,建议不开启，压缩也会消耗cpu，磁盘的话不值钱
rdbcompression yes
# RDB文件名称
dbfilename dump.rdb  
# 文件保存的路径目录
dir ./ 
```

bgsave开始时会fork主进程得到子进程，子进程共享主进程的内存数据。完成fork后读取内存数据并写入 RDB 文件。

fork采用的是copy-on-write技术

- 当主进程执行读操作时，访问共享内存
- 当主进程执行写操作时，则会拷贝一份数据，执行写操作

RDB方式bgsave的基本流程

- fork主进程得到一个子进程，共享内存空间
- 子进程读取内存数据并写入新的RDB文件
- 用新RDB文件替换旧的RDB文件

RDB会在什么时候执行？save 60 1000代表什么含义？

- 默认是服务停止时
- 代表60秒内至少执行1000次修改则触发RDB

RDB的缺点

- RDB执行间隔时间长，两次RDB之间写入数据有丢失的风险
- fork子进程、压缩、写出RDB文件都比较耗时

**AOF**

AOF全称为Append Only File（追加文件）。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文件。

AOF默认是关闭的，需要修改redis.conf配置文件来开启AOF

```shell
# 是否开启AOF功能，默认是no
appendonly yes
# AOF文件的名称
appendfilename "appendonly.aof"
```

AOF的命令记录的频率也可以通过redis.conf文件来配

```shell
# 表示每执行一次写命令，立即记录到AOF文件
appendfsync always 
# 写命令执行完先放入AOF缓冲区，然后表示每隔1秒将缓冲区数据写到AOF文件，是默认方案
appendfsync everysec 
# 写命令执行完先放入AOF缓冲区，由操作系统决定何时将缓冲区内容写回磁盘
appendfsync no
```

|  配置项  |   刷盘时机   |          优点          |             缺点             |
| :------: | :----------: | :--------------------: | :--------------------------: |
|  Always  |   同步刷盘   | 可靠性高，几乎不丢数据 |          性能影响大          |
| everysec |   每秒刷盘   |        性能适中        |       最多丢失1秒数据        |
|    no    | 操作系统控制 |        性能最好        | 可靠性较差，可能丢失大量数据 |

因为是记录命令，AOF文件会比RDB文件大的多。而且AOF会记录对同一个key的多次写操作，但只有最后一次写操作才有意义。通过执行bgrewriteaof命令，可以让AOF文件执行重写功能，用最少的命令达到相同效果。

Redis也会在触发阈值时自动去重写AOF文件。阈值也可以在redis.conf中配置

```shell
# AOF文件比上次文件 增长超过多少百分比则触发重写
auto-aof-rewrite-percentage 100
# AOF文件体积最小多大以上才触发重写 
auto-aof-rewrite-min-size 64mb 
```

RDB和AOF各有自己的优缺点，如果对数据安全性要求较高，在实际开发中往往会结合两者来使用

|                |                     RDB                      |                             AOF                              |
| :------------: | :------------------------------------------: | :----------------------------------------------------------: |
|   持久化方式   |             定时对整个内存做快照             |                     记录每一次执行的命令                     |
|   数据完整性   |          不完整，两次备份之间会丢失          |                   相对完整，取决于刷盘策略                   |
|    文件大小    |             会有压缩，文件体积小             |                    记录命令，文件体积很大                    |
|  宕机恢复速度  |                     很快                     |                              慢                              |
| 数据恢复优先级 |          低，因为数据完整性不如AOF           |                    高，因为数据完整性更高                    |
|  系统资源占用  |            高，大量CPU和内存消耗             | 低，主要是磁盘IO资源 <br />但AOF重写时会占用大量CPU和内存资源 |
|    使用场景    | 可以容忍数分钟的数据丢失，追求更快的启动速度 |                   对数据安全性要求较高常见                   |

### Redis主从

**数据同步原理**

主从第一次同步是全量同步

master如何判断slave是不是第一次来同步数据？这里会用到两个很重要的概念：

- Replication Id：简称replid，是数据集的标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid
- offset：偏移量，随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新

因此slave做数据同步，必须向master声明自己的replication id 和offset，master才可以判断到底需要同步哪些数据

全量同步的流程

- slave节点请求增量同步
- master节点判断replid，发现不一致，拒绝增量同步
- master将完整内存数据生成RDB，发送RDB到slave
- slave清空本地数据，加载master的RDB
- master将RDB期间的命令记录在repl_baklog，并持续将log中的命令发送给slave
- slave执行接收到的命令，保持与master之间的同步

主从第一次同步是全量同步，但如果slave重启后同步，则执行增量同步

注意：repl_baklog大小有上限，写满后会覆盖最早的数据。如果slave断开时间过久，导致尚未备份的数据被覆盖，则无法基于log做增量同步，只能再次全量同步。

可以从以下几个方面来优化Redis主从集群

- 在master中配置repl-diskless-sync yes启用无磁盘复制，避免全量同步时的磁盘IO
- Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘IO
- 适当提高repl_baklog的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步
- 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力

全量同步和增量同步区别

- 全量同步：master将完整内存数据生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave
- 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave

什么时候执行全量同步

- slave节点第一次连接master节点时
- slave节点断开时间太久，repl_baklog中的offset已经被覆盖时

什么时候执行增量同步

- slave节点断开又恢复，并且在repl_baklog中能找到offset时

### Redis哨兵

**哨兵的作用**

Redis提供了哨兵（Sentinel）机制来实现主从集群的自动故障恢复。

- 监控：Sentinel会不断检查您的master和slave是否按预期工作
- 自动故障恢复：如果master故障，Sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主
- 通知：Sentinel充当Redis客户端的服务发现来源，当集群发生故障转移时，会将最新信息推送给Redis的客户端

**服务状态监控**

Sentinel基于心跳机制监测服务状态，每隔1秒向集群的每个实例发送ping命令

- 主观下线：如果某sentinel节点发现某实例未在规定时间响应，则认为该实例**主观下线**。
- 客观下线：若超过指定数量（quorum）的sentinel都认为该实例主观下线，则该实例**客观下线**。quorum值最好超过Sentinel实例数量的一半

**选举新的master**

一旦发现master故障，sentinel需要在salve中选择一个作为新的master，选择依据是这样的：

- 首先会判断slave节点与master节点断开时间长短，如果超过指定值（down-after-milliseconds * 10）则会排除该slave节点
- 然后判断slave节点的slave-priority值，越小优先级越高，如果是0则永不参与选举
- 如果slave-prority一样，则判断slave节点的offset值，越大说明数据越新，优先级越高
- 最后是判断slave节点的运行id大小，越小优先级越高

**如何实现故障转移**

当选中了其中一个slave为新的master后（例如slave1），故障的转移的步骤如下：

- sentinel给备选的slave1节点发送slaveof no one命令，让该节点成为master
- sentinel给所有其它slave发送slaveof 192.168.150.101 7002 命令，让这些slave成为新master的从节点，开始从新的master上同步数据。
- 最后，sentinel将故障节点标记为slave，当故障节点恢复后会自动成为新的master的slave节点

**RedisTemplate的哨兵模式**

在Sentinel集群监管下的Redis主从集群，其节点会因为自动故障转移而发生变化，Redis的客户端必须感知这种变化，及时更新连接信息。Spring的RedisTemplate底层利用lettuce实现了节点的感知和自动切换。

入门步骤

1. 在pom文件中引入redis的starter依赖

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
```

2. 然后在配置文件application.yml中指定sentinel相关信息

```yaml
spring:
  redis:
    sentinel:
      master: mymaster
      nodes:
        - 192.168.146.132:27001
        - 192.168.146.132:27002
        - 192.168.146.132:27003
```

3. 配置主从读写分离

```java
@Bean
public LettuceClientConfigurationBuilderCustomizer clientConfigurationBuilderCustomizer() {
    return clientConfigurationBuilder -> clientConfigurationBuilder.readFrom(ReadFrom.REPLICA_PREFERRED);
}
```

这里的ReadFrom是配置Redis的读取策略，是一个枚举，包括下面选择：

- MASTER：从主节点读取
- MASTER_PREFERRED：优先从master节点读取，master不可用才读取replica
- REPLICA：从slave（replica）节点读取
- REPLICA _PREFERRED：优先从slave（replica）节点读取，所有的slave都不可用才读取master

### Redis分片集群

**分片集群结构**

主从和哨兵可以解决高可用、高并发读的问题。但是依然有两个问题没有解决

- 海量数据存储问题
- 高并发写的问题

使用分片集群可以解决上述问题，分片集群特征

- 集群中有多个master，每个master保存不同数据
- 每个master都可以有多个slave节点
- master之间通过ping监测彼此健康状态
- 客户端请求可以访问集群任意节点，最终都会被转发到正确节点

**散列插槽**

Redis如何判断某个key应该在哪个实例？

- 将16384个插槽分配到不同的实例
- 根据key的有效部分计算哈希值，对16384取余
- 余数作为插槽，寻找插槽所在实例即可

如何将同一类数据固定的保存在同一个Redis实例？

- 这一类数据使用相同的有效部分，例如key都以{typeId}为前缀

**集群伸缩**

redis-cli --cluster提供了很多操作集群的命令，可以通过redis-cli --cluster help方式查看。

**数据迁移**

利用cluster failover命令可以手动让集群中的某个master宕机，切换到执行cluster failover命令的这个slave节点，实现无感知的数据迁移。

手动的Failover支持三种不同模式

- 缺省：默认的流程，如图1~6歩
- force：省略了对offset的一致性校验
- takeover：直接执行第5歩，忽略数据一致性、忽略master状态和其它master的意见

**RedisTemplate访问分片集群**

RedisTemplate底层同样基于lettuce实现了分片集群的支持，而使用的步骤与哨兵模式基本一致：

1. 引入redis的starter依赖
2. 配置分片集群地址
3. 配置读写分离

与哨兵模式相比，其中只有分片集群的配置方式略有差异。

## 多级缓存

### 传统缓存与多级缓存

传统缓存的问题

传统的缓存策略一般是请求到达Tomcat后，先查询Redis，如果未命中则查询数据库，存在下面的问题

- 请求要经过Tomcat处理，Tomcat的性能成为整个系统的瓶颈
- Redis缓存失效时，会对数据库产生冲击

多级缓存方案

多级缓存就是充分利用请求处理的每个环节，分别添加缓存，减轻Tomcat压力，提升服务性能。

用作缓存的Nginx是业务Nginx，需要部署为集群，再有专门的Nginx用来做反向代理。

### JVM进程缓存

**本地进程缓存**

缓存在日常开发中启动至关重要的作用，由于是存储在内存中，数据的读取速度是非常快的，能大量减少对数据库的访问，减少数据库的压力。我们把缓存分为两类：

分布式缓存，例如Redis

- 优点：存储容量更大、可靠性更好、可以在集群间共享
- 缺点：访问缓存有网络开销
- 场景：缓存数据量较大、可靠性要求较高、需要在集群间共享

进程本地缓存，例如HashMap、GuavaCache

- 优点：读取本地内存，没有网络开销，速度更快
- 缺点：存储容量有限、可靠性较低、无法共享
- 场景：性能要求较高，缓存数据量较小

Caffeine是一个基于Java8开发的，提供了近乎最佳命中率的高性能的本地缓存库。目前Spring内部的缓存使用的就是Caffeine。GitHub地址：https://github.com/ben-manes/caffeine

Caffeine提供了三种缓存驱逐策略

- 基于容量：设置缓存的数量上限
- 基于时间：设置缓存的有效时间
- 基于引用：设置缓存为软引用或弱引用，利用GC来回收缓存数据。性能较差，不建议使用

在默认情况下，当一个缓存元素过期的时候，Caffeine不会自动立即将其清理和驱逐。而是在一次读或写操作后，或者在空闲时间完成对失效数据的驱逐。

### Lua语法入门

**初识Lua**

Lua是一种轻量小巧的脚本语言，用标准C语言编写并以源代码形式开放， 其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。官网：https://www.lua.org/

**数据类型**

| 数据类型 |                             描述                             |
| :------: | :----------------------------------------------------------: |
|   nil    | 这个最简单，只有值nil属于该类，表示一个无效值（在条件表达式中相当于false）。 |
| boolean  |                   包含两个值：false和true                    |
|  number  |                   表示双精度类型的实浮点数                   |
|  string  |               字符串由一对双引号或单引号来表示               |
| function |                   由 C  或 Lua  编写的函数                   |
|  table   | Lua 中的表（table）其实是一个"关联数组"（associative arrays），数组的索引可以是数字、字符串或表类型。在  Lua  里，table  的创建是通过"构造表达式"来完成，最简单构造表达式是{}，用来创建一个空表。 |

**变量**

Lua声明变量的时候，并不需要指定数据类型

```lua
-- 声明字符串
local str = 'hello'
-- 声明数字
local num = 21
-- 声明布尔类型
local flag = true
-- 声明数组 key为索引的 table
local arr = {'java', 'python', 'lua'}
-- 声明table，类似java的map
local map =  {name='Jack', age=21}
```

访问table

```lua
-- 访问数组，lua数组的角标从1开始
print(arr[1])
-- 访问table
print(map['name'])
print(map.name)
```

**循环**

数组、table都可以利用for循环来遍历

- 遍历数组

```lua
-- 声明数组 key为索引的 table
local arr = {'java', 'python', 'lua'}
-- 遍历数组
for index,value in ipairs(arr) do
    print(index, value) 
end
```

- 遍历table

```lua
-- 声明map，也就是table
local map = {name='Jack', age=21}
-- 遍历table
for key,value in pairs(map) do
   print(key, value) 
end
```

**函数**

定义函数的语法

```lua
function 函数名( argument1, argument2..., argumentn)
    -- 函数体
    return 返回值
end
```

例如，定义一个函数，用来打印数组

```lua
function printArr(arr)
    for index, value in ipairs(arr) do
        print(value)
    end
end
```

**条件控制**

类似Java的条件控制，例如if、else语法

```lua
if(布尔表达式)
then
   --[ 布尔表达式为 true 时执行该语句块 --]
else
   --[ 布尔表达式为 false 时执行该语句块 --]
end
```

与java不同，布尔表达式中的逻辑运算是基于英文单词

| 操作符 |                             描述                             |          实例           |
| :----: | :----------------------------------------------------------: | :---------------------: |
|  and   |   逻辑与操作符。  若 A 为  false，则返回  A，否则返回  B。   |  (A and B) 为  false。  |
|   or   |   逻辑或操作符。  若 A 为  true，则返回  A，否则返回  B。    |   (A or B) 为  true。   |
|  not   | 逻辑非操作符。与逻辑运算结果相反，如果条件为  true，逻辑非为  false。 | not(A and B) 为  true。 |

### 初识OpenResty

OpenResty®是一个基于Nginx的高性能 Web 平台，用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。具备下列特点：

- 具备Nginx的完整功能
- 基于Lua语言进行扩展，集成了大量精良的 Lua 库、第三方模块
- 允许使用Lua自定义业务逻辑、自定义库

官方网站： https://openresty.org/cn/

**需求**：在OpenResty中接收这个请求，并返回一段商品的假数据。

修改nginx.conf文件

1. 在nginx.conf的http下面，添加对OpenResty的Lua模块的加载

```nginx
#lua 模块
lua_package_path "/usr/local/openresty/lualib/?.lua;;";
#c模块     
lua_package_cpath "/usr/local/openresty/lualib/?.so;;";  
```

2. 在nginx.conf的server下面，添加对/api/item这个路径的监听

```nginx
location /api/item {
    # 响应类型，这里返回json
    default_type application/json;
    # 响应数据由 lua/item.lua这个文件来决定
    content_by_lua_file lua/item.lua;
}
```

编写item.lua文件

1. 在nginx目录创建文件夹：lua
2. 在lua文件夹下，新建文件：item.lua
3. 内容

```nginx
-- 返回假数据，这里的ngx.say()函数，就是写数据到Response中
ngx.say('{"id":10001,"name":"SALSA AIR","title":"RIMOWA 26寸托运箱拉杆箱 SALSA AIR系列果绿色 820.70.36.4","price":19900,"image":"https://m.360buyimg.com/mobilecms/s720x720_jfs/t6934/364/1195375010/84676/e9f2c55f/597ece38N0ddcbc77.jpg!q70.jpg.webp","category":"拉杆箱","brand":"RIMOWA","spec":"","status":1,"createTime":"2019-04-30T16:00:00.000+00:00","updateTime":"2019-04-30T16:00:00.000+00:00","stock":2999,"sold":31290}')
```

4. 重新加载配置

```shell
nginx -s reload
```

**OpenResty获取请求参数**

OpenResty提供了各种API用来获取不同类型的请求参数

|   参数格式   |   参数示例    | 参数解析代码示例                                             |
| :----------: | :-----------: | :----------------------------------------------------------- |
|  路径占位符  |  /item/1001   | -- 正则表达式匹配<br />location ~ /item/(\d+) {<br />   content_by_lua_file lua/item.lua;<br />}<br />-- 匹配到的参数会存入ngx.var数组中，可以用角标获取<br />local id = ngx.var[1] |
|    请求头    |   id：1001    | -- 获取请求头，返回值是table类型  <br />local headers = ngx.req.get_headers() |
| Get请求参数  |   ?id=1001    | -- 获取GET请求参数，返回值是table类型  <br />local getParams = ngx.req.get_uri_args() |
| Post表单参数 |    id=1001    | -- 读取请求体  <br />ngx.req.read_body() <br /> -- 获取POST表单参数，返回值是table类型  <br />local postParams = ngx.req.get_post_args() |
|   JSON参数   | {"id":  1001} | -- 读取请求体  <br />ngx.req.read_body()  <br />-- 获取body中的json参数，返回值是string类型  <br />local jsonBody = ngx.req.get_body_data() |

**nginx内部发送Http请求**

nginx提供了内部API用以发送http请求

```lua
local resp = ngx.location.capture("/path",{
    method = ngx.HTTP_GET,   -- 请求方式
    args = {a=1,b=2},  -- get方式传参数
    body = "c=3&d=4" -- post方式传参数
})
```

返回的响应内容包括：

- resp.status：响应状态码
- resp.header：响应头，是一个table
- resp.body：响应体，就是响应数据

注意：这里的path是路径，并不包含IP和端口。这个请求会被nginx内部的server监听并处理。

但是我们希望这个请求发送到Tomcat服务器，所以还需要编写一个server来对这个路径做反向代理：

```nginx
 location /path {
     # 这里是windows电脑的ip和Java服务端口，需要确保windows防火墙处于关闭状态
     proxy_pass http://192.168.146.1:8081; 
 }
```

**封装http查询的函数**

我们可以把http查询的请求封装为一个函数，放到OpenResty函数库中，方便后期使用。

- 在/usr/local/openresty/lualib目录下创建common.lua文件
- 在common.lua中封装http查询的函数

```lua
-- 封装函数，发送http请求，并解析响应
local function read_http(path, params)
    local resp = ngx.location.capture(path,{
        method = ngx.HTTP_GET,
        args = params,
    })
    if not resp then
        -- 记录错误信息，返回404
        ngx.log(ngx.ERR, "http not found, path: ", path , ", args: ", args)
        ngx.exit(404)
    end
    return resp.body
end
-- 将方法导出
local _M = {  
    read_http = read_http
}  
return _M
```

**使用Http函数查询数据**

我们刚才已经把http查询的请求封装为一个函数，放到OpenResty函数库中，接下来就可以使用这个库了。

- 修改item.lua文件

```lua
-- 导入common函数库
local common = require('common')
local read_http = common.read_http
-- 导入cjson库
local cjson = require('cjson')
-- 获取路径参数
local id = ngx.var[1]

-- 查询商品信息
local itemJSON = read_http("/item/" .. id, nil)
-- 查询库存信息
local stockJSON = read_http("/item/stock/" .. id, nil)

-- JSON转化为lua的table
local item = cjson.decode(itemJSON)
local stock = cjson.decode(stockJSON)
-- 组合数据
item.stock = stock.stock
item.sold = stock.sold

-- 把item序列化为json 返回结果
ngx.say(cjson.encode(item))
```

查询到的是商品、库存的json格式数据，我们需要将两部分数据组装，需要用到JSON处理函数库。

**JSON结果处理**

OpenResty提供了一个cjson的模块用来处理JSON的序列化和反序列化。

官方地址： https://github.com/openresty/lua-cjson/

- 引入cjson模块

```lua
local cjson = require "cjson"
```

- 序列化

```lua
local obj = {
    name = 'jack',
    age = 21
}
local json = cjson.encode(obj)
```

- 反序列化

```lua
local json = '{"name": "jack", "age": 21}'
-- 反序列化
local obj = cjson.decode(json);
print(obj.name)
```

### Redis缓存预热

**冷启动与缓存预热**

冷启动：服务刚刚启动时，Redis中并没有缓存，如果所有商品数据都在第一次查询时添加缓存，可能会给数据库带来较大压力。

缓存预热：在实际开发中，我们可以利用大数据统计用户访问的热点数据，在项目启动时将这些热点数据提前查询并保存到Redis中。

我们数据量较少，可以在启动时将所有数据都放入缓存中。

**缓存预热**

1. 利用Docker安装Redis
2. 在服务中引入Redis依赖
3. 配置Redis地址
4. 编写初始化类

```java
@Component
public class RedisHandler implements InitializingBean {
    @Autowired
    private StringRedisTemplate redisTemplate;

    @Autowired
    private IItemService itemService;

    @Autowired
    private IItemStockService stockService;

    private static final ObjectMapper MAPPER = new ObjectMapper();

    @Override
    public void afterPropertiesSet() throws Exception {
        // 初始化缓存
        List<Item> itemList = itemService.list();
        for (Item item : itemList) {
            String json = MAPPER.writeValueAsString(item);
            redisTemplate.opsForValue().set("item:id:" + item.getId(),json);
        }

        List<ItemStock> stockList = stockService.list();
        for (ItemStock stock : stockList) {
            String json = MAPPER.writeValueAsString(stock);
            redisTemplate.opsForValue().set("item:stock:id:" + stock.getId(),json);
        }
    }
}
```

### OpenResty的Redis模块

OpenResty提供了操作Redis的模块，我们只要引入该模块就能直接使用。

- 引入Redis模块，并初始化Redis对象

```lua
-- 引入redis模块
local redis = require("resty.redis")
-- 初始化Redis对象
local red = redis:new()
-- 设置Redis超时时间
red:set_timeouts(1000, 1000, 1000)
```

- 封装函数，用来释放Redis连接，其实是放入连接池

```lua
-- 关闭redis连接的工具方法，其实是放入连接池
local function close_redis(red)  
    local pool_max_idle_time = 10000 -- 连接的空闲时间，单位是毫秒  
    local pool_size = 100 --连接池大小  
    local ok, err = red:set_keepalive(pool_max_idle_time, pool_size)  
    if not ok then  
        ngx.log(ngx.ERR, "放入Redis连接池失败: ", err)  
    end  
end
```

- 封装函数，从Redis读数据并返回

```lua
-- 查询redis的方法 ip和port是redis地址，key是查询的key
local function read_redis(ip, port, key)  
    -- 获取一个连接
    local ok, err = red:connect(ip, port)  
    if not ok then  
        ngx.log(ngx.ERR, "连接redis失败 : ", err)  
        return nil
    end
    -- 查询redis
    local resp, err = red:get(key) 
    -- 查询失败处理
    if not resp then  
        ngx.log(ngx.ERR, "查询Redis失败: ", err, ", key = " , key)
    end  
    --得到的数据为空处理  
    if resp == ngx.null then
        resp = nil
        ngx.log(ngx.ERR, "查询Redis数据为空, key = ", key)
    end  
    close_redis(red)  
    return resp
end 
```

- 修改item.lua，查询商品和库存时都调用read_data这个函数

```lua
-- 导入common函数库
local common = require('common')
local read_http = common.read_http
local read_redis = common.read_redis
-- 导入cjson库
local cjson = require('cjson')

-- 封装查询函数
function read_data(key, path, params)
    -- 查询redis
    local resp = read_redis("127.0.0.1",6379,key)
    -- 判断查询结果
    if not resp then
        ngx.log("redis查询失败，去查询http，key：",key)
        -- redis查询失败，去查询http
        resp = read_http(path,params)
    end
    return resp
end

-- 获取路径参数
local id = ngx.var[1]

-- 查询商品信息
local itemJSON = read_data("item:id:" .. id, "/item/" .. id, nil)
-- 查询库存信息
local stockJSON = read_data("item:stock:id:" .. id, "/item/stock/" .. id, nil)

-- JSON转化为lua的table
local item = cjson.decode(itemJSON)
local stock = cjson.decode(stockJSON)
-- 组合数据
item.stock = stock.stock
item.sold = stock.sold

-- 把item序列化为json 返回结果
ngx.say(cjson.encode(item))
```

### Nginx本地缓存

OpenResty为Nginx提供了**shard dict**的功能，可以在nginx的多个worker之间共享数据，实现缓存功能。

- 开启共享字典，在nginx.conf的http下添加配置

```nginx
# 共享字典，也就是本地缓存，名称叫做：item_cache，大小150m
lua_shared_dict item_cache 150m;
```

- 操作共享字典

```lua
-- 获取本地缓存对象
local item_cache = ngx.shared.item_cache
-- 存储, 指定key、value、过期时间，单位s，默认为0代表永不过期
item_cache:set('key', 'value', 1000)
-- 读取
local val = item_cache:get('key')
```

修改后的查询逻辑

```lua
-- 导入common函数库
local common = require('common')
local read_http = common.read_http
local read_redis = common.read_redis
-- 导入cjson库
local cjson = require('cjson')
-- 导入共享词典，本地缓存
local item_cache = ngx.shared.item_cache

-- 封装查询函数
function read_data(key, expire, path, params)
    -- 查询本地缓存
    local val = item_cache:get(key)
    if not val then
        ngx.log(ngx.ERR,"本地缓存查询失败，去查询Redis，key：",key)
        -- 查询redis
        val = read_redis("127.0.0.1",6379,key)
        -- 判断查询结果
        if not val then
            ngx.log(ngx.ERR,"redis查询失败，去查询http，key：",key)
            -- redis查询失败，去查询http
            val = read_http(path,params)
        end
    end
    -- 查询成功，把数据写入本地缓存
    item_cache:set(key,val,expire)
    -- 返回数据
    return val
end

-- 获取路径参数
local id = ngx.var[1]

-- 查询商品信息
local itemJSON = read_data("item:id:" .. id, 1800, "/item/" .. id, nil)
-- 查询库存信息
local stockJSON = read_data("item:stock:id:" .. id, 60, "/item/stock/" .. id, nil)

-- JSON转化为lua的table
local item = cjson.decode(itemJSON)
local stock = cjson.decode(stockJSON)
-- 组合数据
item.stock = stock.stock
item.sold = stock.sold

-- 把item序列化为json 返回结果
ngx.say(cjson.encode(item))
```

### 缓存同步

**缓存同步策略**

缓存数据同步的常见方式有三种：

- 设置有效期：给缓存设置有效期，到期后自动删除。再次查询时更新
  - 优势：简单、方便
  - 缺点：时效性差，缓存过期之前可能不一致
  - 场景：更新频率较低，时效性要求低的业务

- 同步双写：在修改数据库的同时，直接修改缓存
  - 优势：时效性强，缓存与数据库强一致
  - 缺点：有代码侵入，耦合度高；
  - 场景：对一致性、时效性要求较高的缓存数据

- 异步通知：修改数据库时发送事件通知，相关服务监听到通知后修改缓存数据
  - 优势：低耦合，可以同时通知多个缓存服务
  - 缺点：时效性一般，可能存在中间不一致状态
  - 场景：时效性要求一般，有多个服务需要同步

**初识Canal**

Canal [kə'næl]，译意为水道/管道/沟渠，canal是阿里巴巴旗下的一款开源项目，基于Java开发。基于数据库增量日志解析，提供增量数据订阅&消费。GitHub的地址：https://github.com/alibaba/canal

Canal是基于mysql的主从同步来实现的，MySQL主从同步的原理如下：

- MySQL master 将数据变更写入二进制日志( binary log），其中记录的数据叫做binary log events
- MySQL slave 将 master 的 binary log events拷贝到它的中继日志(relay log)
- MySQL slave 重放 relay log 中事件，将数据变更反映它自己的数据

Canal就是把自己伪装成MySQL的一个slave节点，从而监听master的binary log变化。再把得到的变化信息通知给Canal的客户端，进而完成对其它数据库的同步。

**监听Canal**

Canal提供了各种语言的客户端，当Canal监听到binlog变化时，会通知Canal的客户端。

Canal提供了各种语言的客户端，当Canal监听到binlog变化时，会通知Canal的客户端。不过这里我们会使用GitHub上的第三方开源的canal-starter。地址：https://github.com/NormanGyllenhaal/canal-client

引入依赖

```xml
<dependency>
    <groupId>top.javatool</groupId>
    <artifactId>canal-spring-boot-starter</artifactId>
    <version>1.2.1-RELEASE</version>
</dependency>
```

编写配置

```yaml
canal:
  destination: heima
  server: 192.168.146.132:11111
```

编写监听器，监听Canal消息

```java
@CanalTable("tb_item")
@Component
public class ItemHandler implements EntryHandler<Item> {
    @Autowired
    private RedisHandler redisHandler;
    @Autowired
    private Cache<Long,Item> itemCache;

    @Override
    public void insert(Item item) {
        // 写数据到JVM进程缓存
        itemCache.put(item.getId(),item);
        // 写数据到redis
        redisHandler.saveItem(item);
    }

    @Override
    public void update(Item before, Item after) {
        // 写数据到JVM进程缓存
        itemCache.put(after.getId(),after);
        // 写数据到redis
        redisHandler.saveItem(after);
    }

    @Override
    public void delete(Item item) {
        // 删除JVM进程缓存数据
        itemCache.invalidate(item.getId());
        // 删除redis中的数据
        redisHandler.deleteItemById(item.getId());
    }
}
```

Canal推送给canal-client的是被修改的这一行数据（row），而我们引入的canal-client则会帮我们把行数据封装到Item实体类中。这个过程中需要知道数据库与实体的映射关系，要用到JPA的几个注解

```java
@Data
@TableName("tb_item")
public class Item {
    @TableId(type = IdType.AUTO)
    @Id
    private Long id;//商品id
    private String name;//商品名称
    private String title;//商品标题
    private Long price;//价格（分）
    private String image;//商品图片
    private String category;//分类名称
    private String brand;//品牌名称
    private String spec;//规格
    private Integer status;//商品状态 1-正常，2-下架
    private Date createTime;//创建时间
    private Date updateTime;//更新时间
    @TableField(exist = false)
    @Transient
    private Integer stock;
    @TableField(exist = false)
    @Transient
    private Integer sold;
}
```

## 服务异步通讯

### MQ的一些常见问题

消息可靠性问题：如何确保发送的消息至少被消费一次

延迟消息问题：如何实现消息的延迟投递

高可用问题：如何避免单点的MQ故障而导致的不可用问题

消息堆积问题：如何解决数百万消息堆积，无法及时消费的问题

### 消息可靠性问题

消息从生产者发送到exchange，再到queue，再到消费者，有哪些导致消息丢失的可能性？

- 发送时丢失
  - 生产者发送的消息未送达exchange
  - 消息到达exchange后未到达queue

- MQ宕机，queue将消息丢失
- consumer接收到消息后未消费就宕机

**生产者确认机制**

RabbitMQ提供了publisher confirm机制来避免消息发送到MQ过程中丢失。消息发送到MQ以后，会返回一个结果给发送者，表示消息是否处理成功。结果有两种请求：

- publisher-confirm，发送者确认
  - 消息成功投递到交换机，返回ack
  - 消息未投递到交换机，返回nack

- publisher-return，发送者回执
  - 消息投递到交换机了，但是没有路由到队列。返回ACK，及路由失败原因

注意：确认机制发送消息时，需要给每个消息设置一个全局唯一id，以区分不同消息，避免ack冲突。

**SpringAMQP实现生产者确认**

1. 在publisher这个微服务的application.yml中添加配置

```yaml
spring:
  rabbitmq:
	publisher-confirm-type: correlated 
    publisher-returns: true 
    template:
      mandatory: true
```

配置说明

- publish-confirm-type：开启publisher-confirm，这里支持两种类型：
  - simple：同步等待confirm结果，直到超时
  - correlated：异步回调，定义ConfirmCallback，MQ返回结果时会回调这个ConfirmCallback

- publish-returns：开启publish-return功能，同样是基于callback机制，不过是定义ReturnCallback
- template.mandatory：定义消息路由失败时的策略。true，则调用ReturnCallback；false：则直接丢弃消息

2. 每个RabbitTemplate只能配置一个ReturnCallback，因此需要在项目启动过程中配置

```java
@Slf4j
@Configuration
public class CommonConfig implements ApplicationContextAware {

    @Override
    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
        // 获取RabbitTemplate对象
        RabbitTemplate rabbitTemplate = applicationContext.getBean(RabbitTemplate.class);
        // 配置ReturnCallback
        rabbitTemplate.setReturnCallback((message, i, s, s1, s2) -> {
            // 记录日志
            log.error("消息发送到队列失败，响应码：{}，失败原因：{}，交换机：{}，路由key：{}，消息：{}",
                     i,s,s1,s2,message.toString());
            // 如果有需要的话，重发消息
        });
    }
}
```

3. 发送消息，指定消息ID、消息ConfirmCallback

```java
@Slf4j
@RunWith(SpringRunner.class)
@SpringBootTest
public class SpringAmqpTest {
    @Autowired
    private RabbitTemplate rabbitTemplate;

    @Test
    public void testSendMessage2SimpleQueue() throws InterruptedException {
        // 1.准备消息
        String message = "hello, spring amqp!";
        // 2.准备CorrelationData
        // 2.1.消息ID
        CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString());
        // 2.2.准备ConfirmCallback
        correlationData.getFuture().addCallback(result -> {
            // 判断结果
            if (result.isAck()) {
                // ACK
                log.debug("消息成功投递到交换机！消息ID：{}",correlationData.getId());
            } else {
                // NACK
                log.error("消息投递到交换机失败！消息ID：{}",correlationData.getId());
                // 重发消息
            }
        }, ex -> {
            // 记录日志
            log.error("消息发送失败！",ex);
            // 重发消息
        });
        // 3.发送消息
        rabbitTemplate.convertAndSend("amq.topic", "simple.test", message,correlationData);
    }
}
```

### 消息持久化

MQ默认是内存存储消息，开启持久化功能可以确保缓存在MQ中的消息不丢失。

- 交换机持久化

```java
@Bean
public DirectExchange simpleDirect() {
    // 三个参数：交换机名称、是否持久化、当没有queue与其绑定时是否自动删除
    return new DirectExchange("simple.direct",true,false);
}
```

- 队列持久化

```java
@Bean
public Queue simpleQueue() {
    // 使用QueueBuilder构建队列，durable就是持久化的
    return QueueBuilder.durable("simple.queue").build();
}
```

- 消息持久化，SpringAMQP中的的消息默认是持久的，可以通过MessageProperties中的DeliveryMode来指定的

```java
@Test
public void testDurableMessage() {
    // 1.准备消息
    Message message = MessageBuilder.withBody("hello, spring".getBytes(StandardCharsets.UTF_8)) // 消息体
            .setDeliveryMode(MessageDeliveryMode.PERSISTENT) // 持久化
            .build();
    // 2.发送消息
    rabbitTemplate.convertAndSend("simple.queue",message);
}
```

### 消费者消息确认

RabbitMQ支持消费者确认机制，即：消费者处理消息后可以向MQ发送ack回执，MQ收到ack回执后才会删除该消息。而SpringAMQP则允许配置三种确认模式：

- manual：手动ack，需要在业务代码结束后，调用api发送ack
- auto：自动ack，由spring监测listener代码是否出现异常，没有异常则返回ack；抛出异常则返回nack
- none：关闭ack，MQ假定消费者获取消息后会成功处理，因此消息投递后立即被删除

配置方式是修改application.yml文件，添加下面配置：

```yaml
spring:
  rabbitmq:
    host: 192.168.146.132 # rabbitMQ的ip地址
    port: 5672 # 端口
    username: root
    password: root
    virtual-host: /
    listener:
      simple:
        prefetch: 1
        acknowledge-mode: auto  # none，关闭ack；manual，手动ack；auto：自动ack
```

### 失败重试机制

**消费者失败重试**

当消费者出现异常后，消息会不断requeue（重新入队）到队列，再重新发送给消费者，然后再次异常，再次requeue，无限循环，导致mq的消息处理飙升，带来不必要的压力。

我们可以利用Spring的retry机制，在消费者出现异常时利用本地重试，而不是无限制的requeue到mq队列。

```yaml
spring:
  rabbitmq:
    listener:
      simple:
        prefetch: 1
        acknowledge-mode: auto  # none，关闭ack；manual，手动ack；auto：自动ack
        retry:
          enabled: true   # 开启消费者失败重试
          initial-interval: 1000   # 初始的失败等待时长为1秒
          multiplier: 3   # 下次失败的等待时长倍数，下次等待时长 = multiplier * last-interval
          max-attempts: 4   # 最大重试次数
          stateless: true   # true无状态；false有状态。如果业务中包含事务，这里改为false
```

**消费者失败消息处理策略**

在开启重试模式后，重试次数耗尽，如果消息依然失败，则需要有MessageRecoverer接口来处理，它包含三种不同的实现：

- RejectAndDontRequeueRecoverer：重试耗尽后，直接reject，丢弃消息。默认就是这种方式
- ImmediateRequeueMessageRecoverer：重试耗尽后，返回nack，消息重新入队
- RepublishMessageRecoverer：重试耗尽后，将失败消息投递到指定的交换机

测试RepublishMessageRecoverer处理模式

- 首先，定义接收失败消息的交换机、队列及其绑定关系
- 然后，定义RepublishMessageRecoverer

```java
@Configuration
public class ErrorMessageConfig {

    @Bean
    public DirectExchange errorMessageExchange() {
        return new DirectExchange("error.direct");
    }

    @Bean
    public Queue errorQueue() {
        return new Queue("error.queue");
    }

    @Bean
    public Binding errorMessageBinding() {
        return BindingBuilder.bind(errorQueue()).to(errorMessageExchange()).with("error");
    }

    @Bean
    public MessageRecoverer republishMessageRecoverer(RabbitTemplate rabbitTemplate) {
        return new RepublishMessageRecoverer(rabbitTemplate,"error.direct","error");
    }
}
```

如何确保RabbitMQ消息的可靠性？

- 开启生产者确认机制，确保生产者的消息能到达队列
- 开启持久化功能，确保消息未消费前在队列中不会丢失
- 开启消费者确认机制为auto，由spring确认消息处理成功后完成ack
- 开启消费者失败重试机制，并设置MessageRecoverer，多次重试失败后将消息投递到异常交换机，交由人工处理

### 死信交换机

**初识死信交换机**

当一个队列中的消息满足下列情况之一时，可以成为死信（dead letter）：

- 消费者使用basic.reject或 basic.nack声明消费失败，并且消息的requeue参数设置为false
- 消息是一个过期消息，超时无人消费
- 要投递的队列消息堆积满了，最早的消息可能成为死信

如果该队列配置了dead-letter-exchange属性，指定了一个交换机，那么队列中的死信就会投递到这个交换机中，而这个交换机称为死信交换机（Dead Letter Exchange，简称DLX）。

如何给队列绑定死信交换机？

- 给队列设置dead-letter-exchange属性，指定一个交换机
- 给队列设置dead-letter-routing-key属性，设置死信交换机与死信队列的RoutingKey

**TTL**

TTL，也就是Time-To-Live。如果一个队列中的消息TTL结束仍未消费，则会变为死信，ttl超时分为两种情况：

- 消息所在的队列设置了存活时间
- 消息本身设置了存活时间

声明一组死信交换机和队列，基于注解方式

```java
@RabbitListener(bindings = @QueueBinding(
        value = @Queue(name = "dl.queue",durable = "true"),
        exchange = @Exchange(name = "dl.direct"),
        key = "dl"
))
public void listenDlQueue(String msg) {
    log.info("消费者接收到了dl.queue的延迟消息");
}
```

要给队列设置超时时间，需要在声明队列时配置x-message-ttl属性

```java
@Configuration
public class TTLMessageConfig {
    @Bean
    public DirectExchange ttlDirectExchange() {
        return new DirectExchange("ttl.direct");
    }

    @Bean
    public Queue ttlQueue() {
        return QueueBuilder
                .durable("ttl.queue")
                .ttl(10000)
                .deadLetterExchange("dl.direct")
                .deadLetterRoutingKey("dl")
                .build();
    }

    @Bean
    public Binding ttlBinding() {
        return BindingBuilder.bind(ttlQueue()).to(ttlDirectExchange()).with("ttl");
    }
}
```

发送消息时，给消息本身设置超时时间

```java
@Test
public void testTTLMessage() {
    // 1.准备消息
    Message message = MessageBuilder.withBody("hello, ttl message".getBytes(StandardCharsets.UTF_8)) // 消息体
            .setDeliveryMode(MessageDeliveryMode.PERSISTENT) // 持久化
            .setExpiration("5000")
            .build();
    // 2.发送消息
    rabbitTemplate.convertAndSend("ttl.direct","ttl",message);
    // 3.记录日志
    log.info("消息已经成功发送了！");
}
```

消息超时的两种方式是？

- 给队列设置ttl属性，进入队列后超过ttl时间的消息变为死信
- 给消息设置ttl属性，队列接收到消息超过ttl时间后变为死信
- 两者共存时，以时间短的ttl为准

如何实现发送一个消息20秒后消费者才收到消息？

- 给消息的目标队列指定死信交换机
- 消费者监听与死信交换机绑定的队列
- 发送消息时给消息设置ttl为20秒

**延迟队列**

利用TTL结合死信交换机，我们实现了消息发出后，消费者延迟收到消息的效果。这种消息模式就称为延迟队列（Delay Queue）模式。

延迟队列的使用场景包括：

- 延迟发送短信
- 用户下单，如果用户在15 分钟内未支付，则自动取消
- 预约工作会议，20分钟后自动通知所有参会人员

延迟队列插件

因为延迟队列的需求非常多，所以RabbitMQ的官方也推出了一个插件，原生支持延迟队列效果。

SpringAMQP使用延迟队列插件

DelayExchange的本质还是官方的三种交换机，只是添加了延迟功能。因此使用时只需要声明一个交换机，交换机的类型可以是任意类型，然后设定delayed属性为true即可。

注解方式声明交换机

```java
@RabbitListener(bindings = @QueueBinding(
        value = @Queue(name = "delay.queue",durable = "true"),
        exchange = @Exchange(name = "delay.direct",delayed = "true"),
        key = "delay"
))
public void listenDelayExchange(String msg) {
    log.info("消费者接收到了delay.queue的延迟消息");
}
```

然后我们向这个delay为true的交换机中发送消息，一定要给消息添加一个header：x-delay，值为延迟的时间，单位为毫秒

```java
@Test
public void testSendDelayMessage() throws InterruptedException {
    // 1.准备消息
    Message message = MessageBuilder.withBody("hello, ttl message".getBytes(StandardCharsets.UTF_8)) // 消息体
            .setDeliveryMode(MessageDeliveryMode.PERSISTENT) // 持久化
            .setHeader("x-delay",5000)
            .build();
    // 2.准备CorrelationData
    CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString());
    // 3.发送消息
    rabbitTemplate.convertAndSend("delay.direct", "delay", message,correlationData);
    log.info("发送消息成功");
}
```

延迟队列插件的使用步骤

- 声明一个交换机，添加delayed属性为true
- 发送消息时，添加x-delay头，值为超时时间

**惰性队列**

消息堆积问题

当生产者发送消息的速度超过了消费者处理消息的速度，就会导致队列中的消息堆积，直到队列存储消息达到上限。最早接收到的消息，可能就会成为死信，会被丢弃，这就是消息堆积问题。

解决消息堆积有三种种思路

- 增加更多消费者，提高消费速度
- 在消费者内开启线程池加快消息处理速度
- 扩大队列容积，提高堆积上限

惰性队列

从RabbitMQ的3.6.0版本开始，就增加了Lazy Queues的概念，也就是惰性队列。

惰性队列的特征

- 接收到消息后直接存入磁盘而非内存
- 消费者要消费消息时才会从磁盘中读取并加载到内存
- 支持数百万条的消息存储

而要设置一个队列为惰性队列，只需要在声明队列时，指定x-queue-mode属性为lazy即可。

可以通过命令行将一个运行中的队列修改为惰性队列

```shel
rabbitmqctl set_policy Lazy "^lazy-queue$" '{"queue-mode":"lazy"}' --apply-to queues  
```

```java
@Bean
public Queue lazyQueue() {
    return QueueBuilder.durable("lazy.queue")
            .lazy()
            .build();
}
```

消息堆积问题的解决方案

- 队列上绑定多个消费者，提高消费速度
- 给消费者开启线程池，提高消费速度
- 使用惰性队列，可以再mq中保存更多消息

惰性队列的优点

- 基于磁盘存储，消息上限高
- 没有间歇性的page-out，性能比较稳定

惰性队列的缺点

- 基于磁盘存储，消息时效性会降低
- 性能受限于磁盘的IO

### MQ集群

**集群分类**

RabbitMQ的是基于Erlang语言编写，而Erlang又是一个面向并发的语言，天然支持集群模式。RabbitMQ的集群有两种模式：

- 普通集群：是一种分布式集群，将队列分散到集群的各个节点，从而提高整个集群的并发能力。
- 镜像集群：是一种主从集群，普通集群的基础上，添加了主从备份功能，提高集群的数据可用性。

镜像集群虽然支持主从，但主从同步并不是强一致的，某些情况下可能有数据丢失的风险。因此在RabbitMQ的3.8版本以后，推出了新的功能：**仲裁队列**来代替镜像集群，底层采用Raft协议确保主从的数据一致性。

**普通集群**

普通集群，或者叫标准集群（classic cluster），具备下列特征：

- 会在集群的各个节点间共享部分数据，包括：交换机、队列元信息。不包含队列中的消息
- 当访问集群某节点时，如果队列不在该节点，会从数据所在节点传递到当前节点并返回
- 队列所在节点宕机，队列中的消息就会丢失

**镜像集群**

镜像集群：本质是主从模式，具备下面的特征：

- 交换机、队列、队列中的消息会在各个mq的镜像节点之间同步备份。
- 创建队列的节点被称为该队列的主节点，备份到的其它节点叫做该队列的镜像节点。
- 一个队列的主节点可能是另一个队列的镜像节点
- 所有操作都是主节点完成，然后同步给镜像节点
- 主宕机后，镜像节点会替代成新的主

**仲裁队列**

仲裁队列：仲裁队列是3.8版本以后才有的新功能，用来替代镜像队列，具备下列特征：

- 与镜像队列一样，都是主从模式，支持主从数据同步
- 使用非常简单，没有复杂的配置
- 主从同步基于Raft协议，强一致
